import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    classification_report,
    roc_curve,
    roc_auc_score
)
import matplotlib.pyplot as plt
import seaborn as sns

# -----------------------------
# Load expression data
# -----------------------------
data = pd.read_csv("../data/raw_expression.csv")

# Same label assumption as before:
# first 20 samples = normal, next 20 = cancer
labels = ["normal"] * 20 + ["cancer"] * 20

# transpose so rows = samples, columns = genes
data = data.T
data["label"] = labels

X = data.drop(columns=["label"])
y = data["label"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train Random Forest model
model = RandomForestClassifier(n_estimators=200, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]  # probability of "cancer"

# ---------------------------------
# 1. Basic metrics
# ---------------------------------
acc = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred, output_dict=True)

print("Accuracy:", acc)
print("\nClassification report:\n")
print(classification_report(y_test, y_pred))

# Save confusion matrix & metrics
pd.DataFrame(cm, index=["normal", "cancer"], columns=["pred_normal", "pred_cancer"])\
  .to_csv("../results/confusion_matrix.csv")
pd.DataFrame({"accuracy": [acc]}).to_csv("../results/model_metrics.csv", index=False)

# ---------------------------------
# 2. Confusion matrix plot
# ---------------------------------
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Normal", "Cancer"],
            yticklabels=["Normal", "Cancer"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.tight_layout()
plt.savefig("../results/confusion_matrix.png")
plt.show()

# ---------------------------------
# 3. ROC curve & AUC
# ---------------------------------
# convert labels to binary (normal=0, cancer=1)
y_binary = (y_test == "cancer").astype(int)

fpr, tpr, thresholds = roc_curve(y_binary, y_proba)
auc_score = roc_auc_score(y_binary, y_proba)
print("ROC AUC:", auc_score)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}")
plt.plot([0, 1], [0, 1], linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.tight_layout()
plt.savefig("../results/roc_curve.png")
plt.show()

# ---------------------------------
# 4. Feature importance (top 20 genes)
# ---------------------------------
importances = model.feature_importances_
genes = X.columns

fi_df = pd.DataFrame({"gene": genes, "importance": importances})
fi_df = fi_df.sort_values("importance", ascending=False).head(20)
fi_df.to_csv("../results/top20_feature_importance.csv", index=False)

plt.figure(figsize=(8,6))
sns.barplot(data=fi_df, x="importance", y="gene")
plt.title("Top 20 Important Genes (Random Forest)")
plt.tight_layout()
plt.savefig("../results/feature_importance_top20.png")
plt.show()
